{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Collecting flask\n",
      "  Downloading Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "Collecting Werkzeug>=2.0\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.9/site-packages (from flask) (8.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.9/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0->flask) (2.0.1)\n",
      "Installing collected packages: Werkzeug, itsdangerous, kafka-python, flask\n",
      "Successfully installed Werkzeug-2.0.2 flask-2.0.2 itsdangerous-2.0.1 kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the required Python 3 dependencies\n",
    "python3 -m pip install kafka-python flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordMetadata(topic='smoke-test', partition=0, topic_partition=TopicPartition(topic='smoke-test', partition=0), offset=5, timestamp=1637859369091, log_start_offset=0, checksum=None, serialized_key_size=-1, serialized_value_size=3, serialized_header_size=-1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "topic =\"smoke-test\"\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
    "# Note: The data you send must be binary\n",
    "producer.send(topic, b\"Hello World!\").get(timeout=30)\n",
    "# Note: The data you send must be binary\n",
    "producer.send(topic, b\"Foo\").get(timeout=30)\n",
    "producer.send(topic, b\"Bar\").get(timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/25 16:56:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5d68dab1-bc22-47aa-abc4-13cbfc6006b3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/11/25 16:56:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>topic</th>\n",
       "      <th>partition</th>\n",
       "      <th>offset</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestampType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>[72, 101, 108, 108, 111, 32, 87, 111, 114, 108...</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-25 14:58:36.122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[70, 111, 111]</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-11-25 14:58:36.148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[66, 97, 114]</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-11-25 14:58:36.150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>[72, 101, 108, 108, 111, 32, 87, 111, 114, 108...</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-11-25 16:56:09.086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>[70, 111, 111]</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-11-25 16:56:09.089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>[66, 97, 114]</td>\n",
       "      <td>smoke-test</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-11-25 16:56:09.091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key                                              value       topic  \\\n",
       "0  None  [72, 101, 108, 108, 111, 32, 87, 111, 114, 108...  smoke-test   \n",
       "1  None                                     [70, 111, 111]  smoke-test   \n",
       "2  None                                      [66, 97, 114]  smoke-test   \n",
       "3  None  [72, 101, 108, 108, 111, 32, 87, 111, 114, 108...  smoke-test   \n",
       "4  None                                     [70, 111, 111]  smoke-test   \n",
       "5  None                                      [66, 97, 114]  smoke-test   \n",
       "\n",
       "   partition  offset               timestamp  timestampType  \n",
       "0          0       0 2021-11-25 14:58:36.122              0  \n",
       "1          0       1 2021-11-25 14:58:36.148              0  \n",
       "2          0       2 2021-11-25 14:58:36.150              0  \n",
       "3          0       3 2021-11-25 16:56:09.086              0  \n",
       "4          0       4 2021-11-25 16:56:09.089              0  \n",
       "5          0       5 2021-11-25 16:56:09.091              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'\n",
    "os.environ['SPARK_OPTS'] = \"--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=warn\"\n",
    "\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\")\n",
    "    .option(\"subscribe\", \"smoke-test\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "smoke_test_query = (\n",
    "    df.writeStream                  # Create an output stream for the query\n",
    "    .outputMode(\"append\")           # Add each msg to end of \"streaming table\"\n",
    "    .format(\"memory\")               # Write output stream to Spark table\n",
    "    .queryName(\"test_query\") \n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait until query runs for a bit\n",
    "sleep(10)\n",
    "\n",
    "# Display status of query\n",
    "display(smoke_test_query.status)\n",
    "\n",
    "# Show result in Jupyter Notebook\n",
    "display(\n",
    "    spark.table(\"test_query\").toPandas()\n",
    ")\n",
    "\n",
    "# Stop query\n",
    "smoke_test_query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
